{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### -----------------------------------------------------------------\n",
    "###           INGRADIENTS: atomic elements\n",
    "### -----------------------------------------------------------------\n",
    "def build_input_v1():\n",
    "    \"\"\"\n",
    "    Return tensor input\n",
    "    \"\"\"\n",
    "    SENTENCE_MAX_LENGTH = 150\n",
    "    tf_X = tf.placeholder(dtype=tf.int64, name='tf_X', shape=[None, SENTENCE_MAX_LENGTH])\n",
    "    tf_y = tf.placeholder(dtype=tf.int64, name='tf_y', shape=[None])\n",
    "    \n",
    "    tf_y0 = tf.reduce_sum(tf.cast(tf.equal(tf_y, 0), 'float'))\n",
    "    tf.summary.scalar(name='y0_count', tensor=tf_y0)\n",
    "    \n",
    "    tf_y1 = tf.reduce_sum(tf.cast(tf.equal(tf_y, 1), 'float'))\n",
    "    tf.summary.scalar(name='y1_count', tensor=tf_y1)\n",
    "    \n",
    "    tf_y2 = tf.reduce_sum(tf.cast(tf.equal(tf_y, 2), 'float'))\n",
    "    tf.summary.scalar(name='y2_count', tensor=tf_y2)\n",
    "    \n",
    "    return tf_X, tf_y\n",
    "\n",
    "\n",
    "def build_input_v2(hyper_parameters={}):\n",
    "    \"\"\"\n",
    "    Return tensor input\n",
    "    Hyper-parameters: SENTENCE_MAX_LENGTH\n",
    "    \"\"\"\n",
    "    if not 'SENTENCE_MAX_LENGTH' in hyper_parameters:\n",
    "        hyper_parameters['SENTENCE_MAX_LENGTH'] = 150\n",
    "    SENTENCE_MAX_LENGTH = hyper_parameters['SENTENCE_MAX_LENGTH']\n",
    "    \n",
    "    tf_X = tf.placeholder(dtype=tf.int64, name='tf_X', shape=[None, SENTENCE_MAX_LENGTH])\n",
    "    tf_y = tf.placeholder(dtype=tf.int64, name='tf_y', shape=[None])\n",
    "    tf_y0 = tf.reduce_sum(tf.cast(tf.equal(tf_y, 0), 'float'))\n",
    "    tf_y1 = tf.reduce_sum(tf.cast(tf.equal(tf_y, 1), 'float'))\n",
    "    tf.summary.scalar(name='y0_count', tensor=tf_y0)\n",
    "    tf.summary.scalar(name='y1_count', tensor=tf_y1)\n",
    "    return tf_X, tf_y\n",
    "\n",
    "\n",
    "def build_inference_v1(tf_X):\n",
    "    def project(tf_X):\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('embedding'):\n",
    "            VOCAB_SIZE = 10000\n",
    "            EMBEDDING_SIZE = 300\n",
    "\n",
    "            tf_word_embeddings = tf.get_variable(name='word_embeddings', dtype=tf.float32,\n",
    "                                              shape=[VOCAB_SIZE, EMBEDDING_SIZE],\n",
    "                                              initializer=tf.truncated_normal_initializer(stddev=5e-2))\n",
    "            tf_projected_sentences = tf.nn.embedding_lookup(params=tf_word_embeddings, ids=tf_X)\n",
    "            return tf_projected_sentences\n",
    "    \n",
    "    tf_projected_sens = project(tf_X)\n",
    "    tf_projected_sens = tf.expand_dims(tf_projected_sens, axis=3)\n",
    "    \n",
    "    with tf.variable_scope('convolution_layer'):\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_projected_sens, filters=10, kernel_size=(5, 5), strides=(2, 2), padding='SAME', name='conv1')\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_after_conv, filters=20, kernel_size=(3, 3), strides=(2, 2), padding='SAME', name='conv2')\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        tf_flatten = tf.layers.flatten(tf_after_conv)\n",
    "        tf_logits = tf.layers.dense(inputs=tf_flatten, units=3, activation=tf.nn.relu)\n",
    "    \n",
    "    return tf_logits\n",
    "\n",
    "\n",
    "def build_inference_v2(tf_X, hyper_parameters={}):\n",
    "    \"\"\"\n",
    "    Hyper-parameters: VOCAB_SIZE, EMBEDDING_SIZE\n",
    "    \"\"\"\n",
    "    def project(tf_X):\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('embedding'):\n",
    "            if not 'VOCAB_SIZE' in hyper_parameters:\n",
    "                hyper_parameters['VOCAB_SIZE'] = 10000\n",
    "            if not 'EMBEDDING_SIZE' in hyper_parameters:\n",
    "                hyper_parameters['EMBEDDING_SIZE'] = 300\n",
    "                \n",
    "            VOCAB_SIZE = hyper_parameters['VOCAB_SIZE']\n",
    "            EMBEDDING_SIZE = hyper_parameters['EMBEDDING_SIZE']\n",
    "\n",
    "            tf_word_embeddings = tf.get_variable(name='word_embeddings', dtype=tf.float32,\n",
    "                                              shape=[VOCAB_SIZE, EMBEDDING_SIZE],\n",
    "                                              initializer=tf.truncated_normal_initializer(stddev=5e-2))\n",
    "            tf_projected_sentences = tf.nn.embedding_lookup(params=tf_word_embeddings, ids=tf_X)\n",
    "            return tf_projected_sentences\n",
    "    \n",
    "    tf_projected_sens = project(tf_X)\n",
    "    tf_projected_sens = tf.expand_dims(tf_projected_sens, axis=3)\n",
    "    \n",
    "    with tf.variable_scope('convolution_layer'):\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_projected_sens, filters=10, kernel_size=(5, 5), strides=(2, 2), padding='SAME', name='conv1')\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_after_conv, filters=20, kernel_size=(3, 3), strides=(2, 2), padding='SAME', name='conv2')\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        tf_flatten = tf.layers.flatten(tf_after_conv)\n",
    "        tf_logits = tf.layers.dense(inputs=tf_flatten, units=3, activation=tf.nn.relu)\n",
    "    \n",
    "    return tf_logits\n",
    "    \n",
    "    \n",
    "def build_inference_v3(tf_X):\n",
    "    def project(tf_X):\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('embedding'):\n",
    "            VOCAB_SIZE = 10000\n",
    "            EMBEDDING_SIZE = 300\n",
    "\n",
    "            tf_word_embeddings = tf.get_variable(name='word_embeddings', dtype=tf.float32,\n",
    "                                              shape=[VOCAB_SIZE, EMBEDDING_SIZE],\n",
    "                                              initializer=tf.truncated_normal_initializer(stddev=5e-2))\n",
    "            tf_projected_sentences = tf.nn.embedding_lookup(params=tf_word_embeddings, ids=tf_X)\n",
    "            return tf_projected_sentences\n",
    "    \n",
    "    tf_projected_sens = project(tf_X)\n",
    "    tf_projected_sens = tf.expand_dims(tf_projected_sens, axis=3)\n",
    "    \n",
    "    with tf.variable_scope('convolution_layer'):\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_projected_sens, filters=100, kernel_size=(5, 5), strides=(2, 2), padding='SAME', name='conv1')\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        tf_flatten = tf.layers.flatten(tf_after_conv)\n",
    "        tf_logits = tf.layers.dense(inputs=tf_flatten, units=100, activation=tf.nn.relu)\n",
    "        tf_logits = tf.layers.dense(inputs=tf_logits, units=3)\n",
    "    \n",
    "    return tf_logits\n",
    "    \n",
    "    \n",
    "def build_loss_v1(tf_logits, tf_y):\n",
    "    tf_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_y, logits=tf_logits)\n",
    "    tf_aggregated_loss = tf.reduce_mean(tf_losses)\n",
    "\n",
    "    tf.summary.scalar(name='loss', tensor=tf_aggregated_loss)\n",
    "    return tf_aggregated_loss\n",
    "\n",
    "def build_optimize_v1(tf_loss):\n",
    "    \"\"\"\n",
    "    Return tensor optimizer and global step\n",
    "    \"\"\"\n",
    "    tf_global_step = tf.get_variable(name='global_step', dtype=tf.int32, shape=(), initializer=tf.zeros_initializer())\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(tf_loss, global_step=tf_global_step)\n",
    "    return optimizer, tf_global_step\n",
    "\n",
    "def build_optimize_v2(tf_loss):\n",
    "    \"\"\"\n",
    "    Return tensor optimizer and global step\n",
    "    \"\"\"\n",
    "    tf_global_step = tf.get_variable(name='global_step', dtype=tf.int32, shape=(), initializer=tf.zeros_initializer())\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "    grads = opt.compute_gradients(tf_loss)\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=tf_global_step)\n",
    "    \n",
    "    with tf.variable_scope('optimize'):\n",
    "        # Add histograms for gradients.\n",
    "        for grad, var in grads:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "                tf.summary.scalar(var.op.name + '/gradients', tf.nn.l2_loss(grad))\n",
    "            else:\n",
    "                logging.warning('Grad is None')\n",
    "    \n",
    "    return apply_gradient_op, tf_global_step\n",
    "\n",
    "\n",
    "def build_optimize_v3(tf_loss):\n",
    "    \"\"\"\n",
    "    Return tensor optimizer and global step\n",
    "    \"\"\"\n",
    "    tf_global_step = tf.get_variable(name='global_step', dtype=tf.int32, shape=(), initializer=tf.zeros_initializer())\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.05)\n",
    "    grads = opt.compute_gradients(tf_loss)\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=tf_global_step)\n",
    "    \n",
    "    with tf.variable_scope('optimize'):\n",
    "        # Add histograms for gradients.\n",
    "        for grad, var in grads:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "                tf.summary.scalar(var.op.name + '/gradients', tf.nn.l2_loss(grad))\n",
    "            else:\n",
    "                logging.warning('Grad is None')\n",
    "    \n",
    "    return apply_gradient_op, tf_global_step\n",
    "\n",
    "\n",
    "def build_optimize_v4(tf_loss):\n",
    "    \"\"\"\n",
    "    Return tensor optimizer and global step\n",
    "    \"\"\"\n",
    "    tf_global_step = tf.get_variable(name='global_step', dtype=tf.int32, shape=(), initializer=tf.zeros_initializer())\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.005)\n",
    "    grads = opt.compute_gradients(tf_loss)\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=tf_global_step)\n",
    "    \n",
    "    with tf.variable_scope('optimize'):\n",
    "        # Add histograms for gradients.\n",
    "        for grad, var in grads:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "                tf.summary.scalar(var.op.name + '/gradients', tf.nn.l2_loss(grad))\n",
    "            else:\n",
    "                logging.warning('Grad is None')\n",
    "    \n",
    "    return apply_gradient_op, tf_global_step\n",
    "\n",
    "\n",
    "def build_predict_v1(tf_logit):\n",
    "    \"\"\"\n",
    "    Convert from tensor logit to tensor one hot\n",
    "    \"\"\"\n",
    "    tf_predict = tf.argmax(tf_logit, axis=1, name='predict')\n",
    "    \n",
    "    tf_predict0 = tf.reduce_sum(tf.cast(tf.equal(tf_predict, 0), 'float'))\n",
    "    tf.summary.scalar(name='predict0_count', tensor=tf_predict0)\n",
    "    \n",
    "    tf_predict1 = tf.reduce_sum(tf.cast(tf.equal(tf_predict, 1), 'float'))\n",
    "    tf.summary.scalar(name='predict1_count', tensor=tf_predict1)\n",
    "    \n",
    "    tf_predict2 = tf.reduce_sum(tf.cast(tf.equal(tf_predict, 2), 'float'))\n",
    "    tf.summary.scalar(name='predict2_count', tensor=tf_predict2)\n",
    "    \n",
    "    # ------------------------\n",
    "    tf_predict_mean0 = tf.reduce_mean(tf_logit[:, 0])\n",
    "    tf.summary.scalar(name='predict_mean0', tensor=tf_predict_mean0)\n",
    "    tf.summary.histogram('predict_0_hist', tf_logit[:, 0])\n",
    "    \n",
    "    tf_predict_mean1 = tf.reduce_mean(tf_logit[:, 1])\n",
    "    tf.summary.scalar(name='predict_mean1', tensor=tf_predict_mean1)\n",
    "    tf.summary.histogram('predict_1_hist', tf_logit[:, 1])\n",
    "    \n",
    "    tf_predict_mean2 = tf.reduce_mean(tf_logit[:, 2])\n",
    "    tf.summary.scalar(name='predict_mean2', tensor=tf_predict_mean2)\n",
    "    tf.summary.histogram('predict_2_hist', tf_logit[:, 2])\n",
    "    \n",
    "    return tf_predict\n",
    "\n",
    "def build_accuracy_v1(tf_predict, tf_Y):\n",
    "    tf_acc = tf.reduce_mean(tf.cast(tf.equal(tf_predict, tf_Y), 'float'), name='accuracy')\n",
    "    tf.summary.scalar(name='accuracy', tensor=tf_acc)\n",
    "    return tf_acc\n",
    "\n",
    "def training_block(graph, tf_X, tf_y, tf_optimizer, tf_global_step, training_generator, test_generator):\n",
    "    \n",
    "    with graph.as_default() as gr:\n",
    "        tf_all_summary = tf.summary.merge_all()\n",
    "        \n",
    "        current_dir = os.getcwd()\n",
    "        experiment_name = datetime.strftime(datetime.now(), '%Y-%m-%dT%H:%M:%S')\n",
    "        tf_train_writer = tf.summary.FileWriter(logdir=os.path.join(current_dir, 'summary', 'train_' + experiment_name), graph=graph)\n",
    "        tf_test_writer = tf.summary.FileWriter(logdir=os.path.join(current_dir, 'summary', 'test_' + experiment_name), graph=graph)\n",
    "        \n",
    "        with tf.Session().as_default() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            SUMMARY_STEP = 10\n",
    "            EVALUATION_STEP = 10\n",
    "            for X, y in training_generator:\n",
    "                feed_dict = {tf_X: X, tf_y: y}\n",
    "                _, global_step = sess.run([tf_optimizer, tf_global_step], feed_dict=feed_dict)\n",
    "                \n",
    "                if global_step % SUMMARY_STEP == 0:\n",
    "                    logging.debug('Collect summary data at step: %s', global_step)\n",
    "                    train_summary_data = sess.run(tf_all_summary, feed_dict=feed_dict)\n",
    "                    tf_train_writer.add_summary(train_summary_data, global_step=global_step)\n",
    "                    \n",
    "                if global_step % EVALUATION_STEP == 0:\n",
    "                    logging.debug('Evaluate at step: %s', global_step)\n",
    "                    X_test, y_test = next(test_generator)\n",
    "                    \n",
    "                    test_summary_data = sess.run(tf_all_summary, feed_dict={\n",
    "                        tf_X: X_test,\n",
    "                        tf_y: y_test\n",
    "                    })\n",
    "                    tf_test_writer.add_summary(test_summary_data, global_step=global_step)\n",
    "            tf_train_writer.flush()\n",
    "            tf_test_writer.flush()\n",
    "            \n",
    "def get_training_generator():\n",
    "    X = np.random.randint(1000, size=(128, 150))\n",
    "    y = np.random.randint(3, size=(128))\n",
    "    for i in range(1000):\n",
    "        yield X, y\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pickle\n",
    "from text2vector import Text2Vector\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pickle\n",
    "from text2vector import Text2Vector\n",
    "import re\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data, batch_size, repeat, shuffle_buffer_size):\n",
    "        \n",
    "        # data: list of tuple, because of the requirement of shuffling\n",
    "        self.data = data\n",
    "        self.__batch_size = batch_size\n",
    "        self.__repeat = repeat\n",
    "        self.__shuffle_buffer_size = shuffle_buffer_size\n",
    "        self.__iterator = None\n",
    "        \n",
    "    def map(self, foo):\n",
    "        new_data = [foo(data_point) for data_point in self.data]\n",
    "        new_dataset = Dataset(new_data, self.__batch_size, self.__repeat, self.__shuffle_buffer_size)\n",
    "        return new_dataset\n",
    "    \n",
    "    def batch(self, batch_size):\n",
    "        return Dataset(self.data, batch_size, self.__repeat, self.__shuffle_buffer_size)\n",
    "    \n",
    "    def repeat(self, count):\n",
    "        return Dataset(self.data, self.__batch_size, count, self.__shuffle_buffer_size)\n",
    "    \n",
    "    def shuffle(self, buffer_size):\n",
    "        return Dataset(self.data, self.__batch_size, self.__repeat, buffer_size)\n",
    "    \n",
    "    def padded_batch(self, batch_size, list_lengths, padded_value):\n",
    "        if isinstance(list_lengths, int):\n",
    "            length = list_lengths\n",
    "            new_data = [list(item[:length]) + [padded_value]*(length - len(item)) for item in self.data]\n",
    "        elif isinstance(list_lengths, tuple):\n",
    "            new_data = []\n",
    "            for datapoint in self.data:\n",
    "                new_datapoint = []\n",
    "                for idx, length in enumerate(list_lengths):\n",
    "                    if length is not None:\n",
    "                        new_datapoint.append(list(datapoint[idx][:length]) + [padded_value]*(length - len(datapoint[idx])))\n",
    "                    else:\n",
    "                        new_datapoint.append(datapoint[idx])\n",
    "                new_data.append(tuple(new_datapoint))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        new_dataset = Dataset(new_data, batch_size, self.__repeat, self.__shuffle_buffer_size)\n",
    "        return new_dataset\n",
    "    \n",
    "    def get_iterator(self):\n",
    "        data_length = len(self.data)\n",
    "        self.__do_estimate_number_steps()\n",
    "        for i in range(self.__repeat):\n",
    "            for j in range(0, data_length-self.__batch_size+1, self.__batch_size):\n",
    "                sample = self.data[j : j + self.__shuffle_buffer_size]\n",
    "                shuffle(sample)\n",
    "                self.data[j : j + self.__shuffle_buffer_size] = sample\n",
    "                start = j\n",
    "                end = j+self.__batch_size\n",
    "                yield tuple(zip(*self.data[start: end]))\n",
    "    \n",
    "    def get_data_length(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    @staticmethod    \n",
    "    def from_csv(filename, columns=None):\n",
    "        df = pd.read_csv(filename)\n",
    "        if isinstance(columns, list):\n",
    "            datas = [list(df[col]) for col in columns]\n",
    "        else:\n",
    "            datas = [list(df[col]) for col in df.columns]\n",
    "        datas = tuple(datas)\n",
    "        Dataset.__assert_valid_data(datas)\n",
    "        \n",
    "        new_dataset = Dataset(list(zip(*datas)), 1, 1, 1)\n",
    "        return new_dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def from_tensor_slices(tensors):\n",
    "        Dataset.__assert_valid_data(tensors)\n",
    "        new_tensors = []\n",
    "        for i in range(len(tensors)):\n",
    "            new_tensors.append(list(tensors[i]))\n",
    "        new_dataset = Dataset(list(zip(*new_tensors)), 1, 1, 1)\n",
    "        return new_dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        data, batch_size, repeat, shuffle_buffer_size = pickle.load(open(filename, 'rb'))\n",
    "        return Dataset(data, batch_size, repeat, shuffle_buffer_size)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        pickle.dump((self.data, self.__batch_size, self.__repeat, self.__shuffle_buffer_size), open(filename, 'wb'))\n",
    "    \n",
    "    @staticmethod\n",
    "    def __assert_valid_data(data):\n",
    "        if not isinstance(data, tuple):\n",
    "            raise DatasetException('data must be a tuple')\n",
    "        for i in range(len(data) - 1):\n",
    "            if len(data[i]) != len(data[i+1]):\n",
    "                raise DatasetException('All element must have the same length. Length of element {}-th is {}, \\\n",
    "                                       length of element {}-th is {}'.format(i, len(data[i]), i+1, len(data[i+1])))\n",
    "    def __do_estimate_number_steps(self):\n",
    "        num_steps_per_epoch = int(self.get_data_length()/self.__batch_size)\n",
    "        logging.info('There will be {} step/epoch'.format(num_steps_per_epoch))\n",
    "        logging.info('There will be total {} steps'.format(num_steps_per_epoch*self.__repeat))\n",
    "        \n",
    "    class DatasetException(Exception):\n",
    "        def __init__(self, message):\n",
    "            Exception.__init__(self, message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pickle\n",
    "from text2vector import Text2Vector\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dataset=None, batch_size=1, repeat=1, shuffle_buffer_size=1):\n",
    "        if dataset is None:\n",
    "            self.data = [] # generator\n",
    "        else:\n",
    "            self.data = dataset.data\n",
    "        self.__batch_size = batch_size\n",
    "        self.__repeat = repeat\n",
    "        self.__shuffle_buffer_size = shuffle_buffer_size\n",
    "        self.__iterator = None\n",
    "        \n",
    "    def map(self, foo):\n",
    "        new_dataset = Dataset(None, self.__batch_size, self.__repeat)\n",
    "        new_dataset.data = [foo(data_point) for data_point in self.data]\n",
    "        return new_dataset\n",
    "    \n",
    "    def batch(self, batch_size):\n",
    "        return Dataset(self, batch_size, self.__repeat)\n",
    "    \n",
    "    def repeat(self, count):\n",
    "        return Dataset(self, self.__batch_size, count)\n",
    "    \n",
    "    def shuffle(self, buffer_size):\n",
    "        return Dataset(self, self.__batch_size, self.__repeat, buffer_size)\n",
    "    \n",
    "    def padded_batch(self, batch_size, list_lengths, padded_value):\n",
    "        if isinstance(list_lengths, int):\n",
    "            length = list_lengths\n",
    "            new_data = [list(item[:length]) + [padded_value]*(length - len(item)) for item in self.data]\n",
    "        elif isinstance(list_lengths, tuple):\n",
    "            new_data = []\n",
    "            for datapoint in self.data:\n",
    "                new_datapoint = []\n",
    "                for idx, length in enumerate(list_lengths):\n",
    "                    if length is not None:\n",
    "                        new_datapoint.append(list(datapoint[idx][:length]) + [padded_value]*(length - len(datapoint[idx])))\n",
    "                    else:\n",
    "                        new_datapoint.append(datapoint[idx])\n",
    "                new_data.append(tuple(new_datapoint))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        new_dataset = Dataset(None, batch_size, self.__repeat)\n",
    "        new_dataset.data = new_data\n",
    "        return new_dataset\n",
    "    \n",
    "    def get_iterator(self):\n",
    "        data_length = len(self.data)\n",
    "        for i in range(self.__repeat):\n",
    "            for j in range(0, data_length-self.__batch_size+1, self.__batch_size):\n",
    "                sample = self.data[j : j + self.__shuffle_buffer_size]\n",
    "                shuffle(sample)\n",
    "                self.data[j : j + self.__shuffle_buffer_size] = sample\n",
    "                start = j\n",
    "                end = j+self.__batch_size\n",
    "                yield self.data[start: end]\n",
    "\n",
    "    @staticmethod    \n",
    "    def from_csv(filename, columns=None):\n",
    "        df = pd.read_csv(filename)\n",
    "        if isinstance(columns, list):\n",
    "            datas = [list(df[col]) for col in columns]\n",
    "        else:\n",
    "            datas = [list(df[col]) for col in df.columns]\n",
    "        new_dataset = Dataset()\n",
    "        new_dataset.data = list(zip(*datas))\n",
    "        return new_dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def from_tensor_slices(tensors):\n",
    "        assert isinstance(tensors, tuple)\n",
    "        new_dataset = Dataset()\n",
    "        new_dataset.data = list(zip(*tensors))\n",
    "        return new_dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_pickle_file(filename):\n",
    "        return pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    def save(self, filename):\n",
    "        pickle.dump(self, open(filename, 'wb'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec_model = pickle.load(open('text2vec.p', 'rb'))\n",
    "LABEL_MAPPING = {\n",
    "        'positive': 0,\n",
    "        'neutral': 1,\n",
    "        'negative': 2\n",
    "    }\n",
    "def preprocess_text(doc):\n",
    "    doc = doc.lower()\n",
    "    NUMBERS_PATTERN = re.compile(r\"[+-]?\\d+(?:\\.\\d+)?\")\n",
    "    doc = re.sub(NUMBERS_PATTERN, '', doc)\n",
    "    URL_PATTERN = re.compile(\n",
    "            r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))')\n",
    "    doc = re.sub(URL_PATTERN, 'URL', doc)\n",
    "    return doc\n",
    "def standadize_datapoint(datapoint):\n",
    "    doc, label = datapoint\n",
    "    doc = preprocess_text(doc)\n",
    "    return text2vec_model.doc_to_vec([doc])[0], LABEL_MAPPING[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_PATH = os.path.join('/dataset', 'entropy_2018')\n",
    "TRAINING_PATH = os.path.join(ENTROPY_PATH, 'training_set.csv')\n",
    "TEST_PATH = os.path.join(ENTROPY_PATH, 'test_set.csv')\n",
    "\n",
    "dataset = Dataset.from_csv(TRAINING_PATH)\n",
    "dataset = dataset.map(standadize_datapoint)\n",
    "dataset.save('dataset.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pickle_file('dataset.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randint(10, size=(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8],\n",
       "       [6],\n",
       "       [9],\n",
       "       [6],\n",
       "       [2]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_tensor_slices((data,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([8]),), (array([6]),), (array([9]),), (array([6]),), (array([2]),)]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset.from_tensor_slices((data,))\n",
    "# dataset = Dataset.fromCsv(TRAINING_PATH, ['sentiment', 'sentence'])\n",
    "dataset = dataset.padded_batch(2, (3, ), -1)\n",
    "dataset = dataset.repeat(2)\n",
    "# dataset = dataset.shuffle(5)\n",
    "# dataset = dataset.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.get_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-4ce711c44abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sentiment', 'sentence']\n",
    "df = pd.read_csv(TRAINING_PATH)\n",
    "if isinstance(columns, list):\n",
    "    datas = [list(df[col] for col in columns]\n",
    "else:\n",
    "    datas = [df[col] for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook dataset.ipynb to script\n",
      "[NbConvertApp] Writing 5348 bytes to dataset.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

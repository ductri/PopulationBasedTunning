{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from datetime import datetime\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_PATH = os.path.join('/home/ductri/code/all_dataset', 'entropy_2018')\n",
    "TRAINING_PATH = os.path.join(ENTROPY_PATH, 'training_set.csv')\n",
    "TEST_PATH = os.path.join(ENTROPY_PATH, 'test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAINING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2Vector:\n",
    "    OUT_OF_VOCAB = 'OUT_OF_VOCAB'\n",
    "    VOCAB_SIZE = 10000\n",
    "    def __init__(self):\n",
    "        self.counts = None\n",
    "        self.int_to_vocab = None\n",
    "        self.vocab_to_int = None\n",
    "\n",
    "    def __tokenize(self, text):\n",
    "        \"\"\"\n",
    "\n",
    "        :param text:\n",
    "        :return: list\n",
    "        \"\"\"\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def doc_to_vec(self, list_documents):\n",
    "        logging.debug('-- From doc_to_vec')\n",
    "        assert isinstance(list_documents, list)\n",
    "        len_list = len(list_documents)\n",
    "        tokenized_documents = []\n",
    "        for i, doc in enumerate(list_documents):\n",
    "            if i % 100 == 0:\n",
    "                logging.debug('--- Tokenizing: {}\\{}, len={}'.format(i, len_list, len(doc)))\n",
    "            tokenized_documents.append(self.__tokenize(doc))\n",
    "\n",
    "        return [self.__transform(doc) for doc in tokenized_documents]\n",
    "\n",
    "    def vec_to_doc(self, list_vecs):\n",
    "        assert isinstance(list_vecs, list) or isinstance(list_vecs, np.ndarray)\n",
    "        return [self.__invert_transform(vec) for vec in list_vecs]\n",
    "\n",
    "    def fit(self, list_texts):\n",
    "        logging.debug('-- From fit')\n",
    "        if self.counts or self.vocab_to_int or self.int_to_vocab:\n",
    "            raise Exception('\"fit\" is a one-time function')\n",
    "        list_tokenized_texts = [self.__tokenize(text) for text in list_texts]\n",
    "        all_tokens = itertools.chain(*list_tokenized_texts)\n",
    "        self.counts = collections.Counter(all_tokens)\n",
    "\n",
    "        self.int_to_vocab = self.__get_vocab(vocab_size=Text2Vector.VOCAB_SIZE-1) # 1 for PADDING\n",
    "        self.int_to_vocab = [Text2Vector.PADDING] + self.int_to_vocab\n",
    "        self.vocab_to_int = {word: index for index, word in enumerate(self.int_to_vocab)}\n",
    "\n",
    "    def __transform(self, list_tokens):\n",
    "        if not self.vocab_to_int:\n",
    "            raise Exception('vocab_to_int is None')\n",
    "\n",
    "        return [self.vocab_to_int[token] if token in self.vocab_to_int else self.vocab_to_int[Text2Vector.OUT_OF_VOCAB] for token in list_tokens]\n",
    "\n",
    "    def __invert_transform(self, list_ints):\n",
    "        \"\"\"\n",
    "\n",
    "        :param list_ints:\n",
    "        :return: A document str\n",
    "        \"\"\"\n",
    "        if not self.int_to_vocab:\n",
    "            raise Exception('vocab_to_int is None')\n",
    "\n",
    "        return ' '.join([self.int_to_vocab[int_item] for int_item in list_ints])\n",
    "\n",
    "    def __get_vocab(self, vocab_size=1):\n",
    "        if not self.counts:\n",
    "            raise Exception('counts is None')\n",
    "        return [item[0] for item in self.counts.most_common(n=vocab_size)]\n",
    "\n",
    "    def get_most_common(self, n=10):\n",
    "        if not self.counts:\n",
    "            raise Exception('counts is None')\n",
    "        return self.counts.most_common(n)\n",
    "\n",
    "    def export_vocab(self, output_file):\n",
    "        pd.DataFrame({'word': self.int_to_vocab}).to_csv(output_file, index=False, header=False)\n",
    "        logging.debug('Exported %s words in vocab into file %s', len(self.int_to_vocab), output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-acdde9199df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText2Vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-270c21a22617>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, list_texts)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_to_int\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_to_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"fit\" is a one-time function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mlist_tokenized_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mall_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist_tokenized_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-270c21a22617>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_to_int\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_to_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"fit\" is a one-time function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mlist_tokenized_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mall_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist_tokenized_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-270c21a22617>\u001b[0m in \u001b[0;36m__tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \"\"\"\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdoc_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Handles parentheses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text2vec = Text2Vector()\n",
    "text2vec.fit(list(df_train['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### -----------------------------------------------------------------\n",
    "###           INGRADIENTS: atomic elements\n",
    "### -----------------------------------------------------------------\n",
    "def build_input_v1():\n",
    "    \"\"\"\n",
    "    Return tensor input\n",
    "    \"\"\"\n",
    "    SENTENCE_MAX_LENGTH = 150\n",
    "    tf_X = tf.placeholder(dtype=tf.int32, name='tf_X', shape=[None, SENTENCE_MAX_LENGTH])\n",
    "    tf_y = tf.placeholder(dtype=tf.int32, name='tf_y', shape=[None])\n",
    "    return tf_X, tf_y\n",
    "\n",
    "def build_inference_v1(tf_X):\n",
    "    def project(tf_X):\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('embedding'):\n",
    "            VOCAB_SIZE = 10000\n",
    "            EMBEDDING_SIZE = 300\n",
    "\n",
    "            tf_word_embeddings = tf.get_variable(name='word_embeddings', dtype=tf.float32,\n",
    "                                              shape=[VOCAB_SIZE, EMBEDDING_SIZE],\n",
    "                                              initializer=tf.truncated_normal_initializer(stddev=5e-2))\n",
    "            tf_projected_sentences = tf.nn.embedding_lookup(params=tf_word_embeddings, ids=tf_X)\n",
    "            return tf_projected_sentences\n",
    "    \n",
    "    tf_projected_sens = project(tf_X)\n",
    "    tf_projected_sens = tf.expand_dims(tf_projected_sens, axis=3)\n",
    "    \n",
    "    with tf.variable_scope('convolution_layer'):\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_projected_sens, filters=10, kernel_size=(5, 5), strides=(2, 2), padding='SAME', name='conv1')\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_after_conv, filters=20, kernel_size=(3, 3), strides=(2, 2), padding='SAME', name='conv2')\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        tf_flatten = tf.layers.flatten(tf_after_conv)\n",
    "        tf_logits = tf.layers.dense(inputs=tf_flatten, units=3, activation=tf.nn.relu)\n",
    "    \n",
    "    return tf_logits\n",
    "    \n",
    "def build_loss_v1(tf_logits, tf_y):\n",
    "    tf_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_y, logits=tf_logits)\n",
    "    tf_aggregated_loss = tf.reduce_mean(tf_losses)\n",
    "\n",
    "    tf.summary.scalar(name='loss', tensor=tf_aggregated_loss)\n",
    "    return tf_aggregated_loss\n",
    "\n",
    "def build_optimize_v1(tf_loss):\n",
    "    \"\"\"\n",
    "    Return tensor optimizer and global step\n",
    "    \"\"\"\n",
    "    tf_global_step = tf.get_variable(name='global_step', dtype=tf.int32, shape=(), initializer=tf.zeros_initializer())\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(tf_loss, global_step=tf_global_step)\n",
    "    return optimizer, tf_global_step\n",
    "\n",
    "def build_predict(tf_logit):\n",
    "    \"\"\"\n",
    "    Convert from tensor logit to tensor one hot\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def training_block(graph, tf_X, tf_y, tf_optimizer, tf_global_step, training_generator, test_generator):\n",
    "    \n",
    "    with graph.as_default() as gr:\n",
    "        tf_all_summary = tf.summary.merge_all()\n",
    "        \n",
    "        current_dir = os.getcwd()\n",
    "        experiment_name = datetime.strftime(datetime.now(), '%Y-%m-%dT%H:%M:%S')\n",
    "        tf_train_writer = tf.summary.FileWriter(logdir=os.path.join(current_dir, 'summary', 'train_' + experiment_name), graph=graph)\n",
    "        tf_test_writer = tf.summary.FileWriter(logdir=os.path.join(current_dir, 'summary', 'test_' + experiment_name), graph=graph)\n",
    "        \n",
    "        with tf.Session().as_default() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            SUMMARY_STEP = 10\n",
    "            EVALUATION_STEP = 10\n",
    "            for X, y in training_generator:\n",
    "                feed_dict = {tf_X: X, tf_y: y}\n",
    "                _, global_step = sess.run([tf_optimizer, tf_global_step], feed_dict=feed_dict)\n",
    "                \n",
    "                if global_step % SUMMARY_STEP == 0:\n",
    "                    logging.debug('Collect summary data at step: %s', global_step)\n",
    "                    train_summary_data = sess.run(tf_all_summary, feed_dict=feed_dict)\n",
    "                    tf_train_writer.add_summary(train_summary_data, global_step=global_step)\n",
    "                    \n",
    "                if global_step % EVALUATION_STEP == 0:\n",
    "                    logging.debug('Evaluate at step: %s', global_step)\n",
    "                    X_test, y_test = next(test_generator)\n",
    "                    \n",
    "                    test_summary_data = sess.run(tf_all_summary, feed_dict={\n",
    "                        tf_X: X_test,\n",
    "                        tf_y: y_test\n",
    "                    })\n",
    "                    tf_test_writer.add_summary(test_summary_data, global_step=global_step)\n",
    "\n",
    "def get_training_generator():\n",
    "    X = np.random.randint(1000, size=(512, 150))\n",
    "    y = np.random.randint(3, size=(512))\n",
    "    for i in range(1000):\n",
    "        yield X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_X, tf_y = build_input_v1()\n",
    "    tf_logit = build_inference_v1(tf_X)\n",
    "    tf_loss = build_loss_v1(tf_logit, tf_y)\n",
    "    tf_optimizer, tf_global_step = build_optimize_v1(tf_loss)\n",
    "    \n",
    "    training_block(graph=graph, tf_X=tf_X, tf_y=tf_y, training_generator=get_training_generator(), \n",
    "                   test_generator=get_training_generator(),\n",
    "                   tf_optimizer=tf_optimizer,\n",
    "                   tf_global_step=tf_global_step)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inference_v2(tf_X):\n",
    "    \"\"\"\n",
    "    ```Thoi kho qua, de thu sau\n",
    "    Return tensor logit\n",
    "    tf_X: [batch_size, sentence_max_length]\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.device('/cpu:0'), tf.variable_scope('embedding'):\n",
    "        VOCAB_SIZE = 10000\n",
    "        EMBEDDING_SIZE = 300\n",
    "        \n",
    "        tf_word_embeddings = tf.get_variable(name='word_embeddings', dtype=tf.float32,\n",
    "                                          shape=[VOCAB_SIZE, EMBEDDING_SIZE],\n",
    "                                          initializer=tf.truncated_normal_initializer(stddev=5e-2))\n",
    "        tf_projected_sentences = tf.nn.embedding_lookup(params=tf_word_embeddings, ids=tf_X)\n",
    "        list_tf_word_embeddings = tf.unstack(tf_projected_sentences, axis=1)\n",
    "        \n",
    "    with tf.variable_scope('LSTM'):\n",
    "        STATE_SIZE = 200\n",
    "        lstm_cell = rnn.BasicLSTMCell(STATE_SIZE, forget_bias=1.0)\n",
    "        # Each output has shape of [batch_size, state_size]\n",
    "        list_outputs, _ = rnn.static_rnn(cell=lstm_cell, inputs=word_embeddings, dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('Attention'):\n",
    "        ATTENTION_SIZE = 200\n",
    "        tf_output = tf.stack(list_outputs, axis=2) # [batch_size, state_size, sentence_max_length]\n",
    "        \n",
    "#         tf_attention_weights =\n",
    "        \n",
    "        tf_after_attention = tf.layers.dense(tf_output, units=ATTENTION_SIZE, activation=tf.nn.relu)\n",
    "        \n",
    "    with tf.variable_scope('Fully-Connected'):\n",
    "        tf_logit = tf.layers.dense(tf_after_attention, units=ATTENTION_SIZE, activation=tf.nn.relu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

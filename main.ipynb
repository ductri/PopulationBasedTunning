{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from datetime import datetime\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "import nltk\n",
    "import collections\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_PATH = os.path.join('/dataset', 'entropy_2018')\n",
    "TRAINING_PATH = os.path.join(ENTROPY_PATH, 'training_set.csv')\n",
    "TEST_PATH = os.path.join(ENTROPY_PATH, 'test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAINING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df_train['sentence']\n",
    "y=df_train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = zip(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NguyenPhong di hok? Voucher co HSD den bao gio vay ad?', 'neutral')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(doc):\n",
    "    doc = doc.lower()\n",
    "    NUMBERS_PATTERN = re.compile(r\"[+-]?\\d+(?:\\.\\d+)?\")\n",
    "    doc = re.sub(NUMBERS_PATTERN, '', doc)\n",
    "    URL_PATTERN = re.compile(\n",
    "            r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))')\n",
    "    doc = re.sub(URL_PATTERN, 'URL', doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2Vector:\n",
    "    OUT_OF_VOCAB = 'OUT_OF_VOCAB'\n",
    "    VOCAB_SIZE = 10000\n",
    "    def __init__(self):\n",
    "        self.counts = None\n",
    "        self.int_to_vocab = None\n",
    "        self.vocab_to_int = None\n",
    "\n",
    "    def __tokenize(self, text):\n",
    "        \"\"\"\n",
    "\n",
    "        :param text:\n",
    "        :return: list\n",
    "        \"\"\"\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def doc_to_vec(self, list_documents):\n",
    "        logging.debug('-- From doc_to_vec')\n",
    "        assert isinstance(list_documents, list)\n",
    "        len_list = len(list_documents)\n",
    "        tokenized_documents = []\n",
    "        \n",
    "        list_documents = [preprocess_text(doc) for doc in list_documents]\n",
    "        for i, doc in enumerate(list_documents):\n",
    "            if i % 100 == 0:\n",
    "                logging.debug('--- Tokenizing: {}\\{}, len={}'.format(i, len_list, len(doc)))\n",
    "            tokenized_documents.append(self.__tokenize(doc))\n",
    "\n",
    "        return [self.__transform(doc) for doc in tokenized_documents]\n",
    "\n",
    "    def vec_to_doc(self, list_vecs):\n",
    "        assert isinstance(list_vecs, list) or isinstance(list_vecs, np.ndarray)\n",
    "        return [self.__invert_transform(vec) for vec in list_vecs]\n",
    "\n",
    "    def fit(self, list_texts):\n",
    "        logging.debug('-- From fit')\n",
    "        if self.counts or self.vocab_to_int or self.int_to_vocab:\n",
    "            raise Exception('\"fit\" is a one-time function')\n",
    "        list_tokenized_texts = [self.__tokenize(text) for text in list_texts]\n",
    "        all_tokens = itertools.chain(*list_tokenized_texts)\n",
    "        self.counts = collections.Counter(all_tokens)\n",
    "\n",
    "        self.int_to_vocab = self.__get_vocab(vocab_size=Text2Vector.VOCAB_SIZE-1) # 1 for PADDING\n",
    "        self.int_to_vocab = self.int_to_vocab + [Text2Vector.OUT_OF_VOCAB]\n",
    "        self.vocab_to_int = {word: index for index, word in enumerate(self.int_to_vocab)}\n",
    "\n",
    "    def __transform(self, list_tokens):\n",
    "        if not self.vocab_to_int:\n",
    "            raise Exception('vocab_to_int is None')\n",
    "\n",
    "        return [self.vocab_to_int[token] if token in self.vocab_to_int else self.vocab_to_int[Text2Vector.OUT_OF_VOCAB] for token in list_tokens]\n",
    "\n",
    "    def __invert_transform(self, list_ints):\n",
    "        \"\"\"\n",
    "\n",
    "        :param list_ints:\n",
    "        :return: A document str\n",
    "        \"\"\"\n",
    "        if not self.int_to_vocab:\n",
    "            raise Exception('vocab_to_int is None')\n",
    "\n",
    "        return ' '.join([self.int_to_vocab[int_item] for int_item in list_ints])\n",
    "\n",
    "    def __get_vocab(self, vocab_size=1):\n",
    "        if not self.counts:\n",
    "            raise Exception('counts is None')\n",
    "        return [item[0] for item in self.counts.most_common(n=vocab_size)]\n",
    "\n",
    "    def get_most_common(self, n=10):\n",
    "        if not self.counts:\n",
    "            raise Exception('counts is None')\n",
    "        return self.counts.most_common(n)\n",
    "\n",
    "    def export_vocab(self, output_file):\n",
    "        pd.DataFrame({'word': self.int_to_vocab}).to_csv(output_file, index=False, header=False)\n",
    "        logging.debug('Exported %s words in vocab into file %s', len(self.int_to_vocab), output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load from saved pickle\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('text2vec.p'):\n",
    "    logging.info('Load from saved pickle')\n",
    "    text2vec = pickle.load(open('text2vec.p', 'rb'))\n",
    "else:\n",
    "    logging.info('Fitting')\n",
    "    text2vec = Text2Vector()\n",
    "    text2vec.fit(list(df_train['sentence']))\n",
    "    pickle.dump(text2vec, open('text2vec.p', 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_datapoint(doc, label):\n",
    "    if not isinstance(doc, str):\n",
    "        doc = doc.decode('utf-8')\n",
    "        \n",
    "    if not isinstance(label, str):\n",
    "        label = label.decode('utf-8')\n",
    "\n",
    "    LABEL_MAPPING = {\n",
    "        'positive': 0,\n",
    "        'neutral': 1,\n",
    "        'negative': 2\n",
    "    }\n",
    "    doc = preprocess_text(doc)\n",
    "    doc = text2vec.doc_to_vec([doc])[0]\n",
    "    label = LABEL_MAPPING[label]\n",
    "    \n",
    "    return (doc, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:-- From doc_to_vec\n",
      "DEBUG:root:--- Tokenizing: 0\\1, len=26\n",
      "DEBUG:root:-- From doc_to_vec\n",
      "DEBUG:root:--- Tokenizing: 0\\1, len=80\n",
      "DEBUG:root:-- From doc_to_vec\n",
      "DEBUG:root:--- Tokenizing: 0\\1, len=54\n",
      "DEBUG:root:-- From doc_to_vec\n",
      "DEBUG:root:--- Tokenizing: 0\\1, len=195\n"
     ]
    },
    {
     "ename": "DataLossError",
     "evalue": "Attempted to pad to a smaller size than the input element.\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,1], [?,1]], output_types=[DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-104-4596c5fb3e1b>\", line 11, in <module>\n    next_element = iterator.get_next()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 373, in get_next\n    name=name)), self._output_types,\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1666, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3417, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1743, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nDataLossError (see above for traceback): Attempted to pad to a smaller size than the input element.\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,1], [?,1]], output_types=[DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mDataLossError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataLossError\u001b[0m: Attempted to pad to a smaller size than the input element.\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,1], [?,1]], output_types=[DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDataLossError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-4596c5fb3e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mnext_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataLossError\u001b[0m: Attempted to pad to a smaller size than the input element.\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,1], [?,1]], output_types=[DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-104-4596c5fb3e1b>\", line 11, in <module>\n    next_element = iterator.get_next()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 373, in get_next\n    name=name)), self._output_types,\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1666, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3417, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1743, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nDataLossError (see above for traceback): Attempted to pad to a smaller size than the input element.\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,1], [?,1]], output_types=[DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        filenames = [TRAINING_PATH]\n",
    "        record_defaults = [tf.string, tf.string]\n",
    "        dataset = tf.contrib.data.CsvDataset(filenames, record_defaults, header=True)\n",
    "        dataset = dataset.map(lambda doc, label: tuple(tf.py_func(preprocess_datapoint, [doc, label], [tf.int64, tf.int64])))\n",
    "        dataset = dataset.padded_batch(4, padded_shapes=(1, 1))\n",
    "\n",
    "#         dataset = dataset.batch(4)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "        x = sess.run(next_element)\n",
    "        y = sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(y, collections.Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUT_OF_VOCAB tri nguyen']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec.vec_to_doc(text2vec.doc_to_vec(['duc tri nguyen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_v2():\n",
    "    \"\"\"\n",
    "    Return tensor input\n",
    "    \"\"\"\n",
    "    SENTENCE_MAX_LENGTH = 150\n",
    "    BATCH_SIZE = 2\n",
    "    \n",
    "    filenames = [TRAINING_PATH]\n",
    "    record_defaults = [tf.string, tf.string]\n",
    "    dataset = tf.contrib.data.CsvDataset(filenames, record_defaults, header=True)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_text)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    tf_X, tf_y = iterator.get_next()\n",
    "\n",
    "    return tf_X, tf_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### -----------------------------------------------------------------\n",
    "###           INGRADIENTS: atomic elements\n",
    "### -----------------------------------------------------------------\n",
    "def build_input_v1():\n",
    "    \"\"\"\n",
    "    Return tensor input\n",
    "    \"\"\"\n",
    "    SENTENCE_MAX_LENGTH = 150\n",
    "    tf_X = tf.placeholder(dtype=tf.int32, name='tf_X', shape=[None, SENTENCE_MAX_LENGTH])\n",
    "    tf_y = tf.placeholder(dtype=tf.int32, name='tf_y', shape=[None])\n",
    "    return tf_X, tf_y\n",
    "\n",
    "def build_inference_v1(tf_X):\n",
    "    def project(tf_X):\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('embedding'):\n",
    "            VOCAB_SIZE = 10000\n",
    "            EMBEDDING_SIZE = 300\n",
    "\n",
    "            tf_word_embeddings = tf.get_variable(name='word_embeddings', dtype=tf.float32,\n",
    "                                              shape=[VOCAB_SIZE, EMBEDDING_SIZE],\n",
    "                                              initializer=tf.truncated_normal_initializer(stddev=5e-2))\n",
    "            tf_projected_sentences = tf.nn.embedding_lookup(params=tf_word_embeddings, ids=tf_X)\n",
    "            return tf_projected_sentences\n",
    "    \n",
    "    tf_projected_sens = project(tf_X)\n",
    "    tf_projected_sens = tf.expand_dims(tf_projected_sens, axis=3)\n",
    "    \n",
    "    with tf.variable_scope('convolution_layer'):\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_projected_sens, filters=10, kernel_size=(5, 5), strides=(2, 2), padding='SAME', name='conv1')\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_after_conv, filters=20, kernel_size=(3, 3), strides=(2, 2), padding='SAME', name='conv2')\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        tf_flatten = tf.layers.flatten(tf_after_conv)\n",
    "        tf_logits = tf.layers.dense(inputs=tf_flatten, units=3, activation=tf.nn.relu)\n",
    "    \n",
    "    return tf_logits\n",
    "    \n",
    "def build_loss_v1(tf_logits, tf_y):\n",
    "    tf_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_y, logits=tf_logits)\n",
    "    tf_aggregated_loss = tf.reduce_mean(tf_losses)\n",
    "\n",
    "    tf.summary.scalar(name='loss', tensor=tf_aggregated_loss)\n",
    "    return tf_aggregated_loss\n",
    "\n",
    "def build_optimize_v1(tf_loss):\n",
    "    \"\"\"\n",
    "    Return tensor optimizer and global step\n",
    "    \"\"\"\n",
    "    tf_global_step = tf.get_variable(name='global_step', dtype=tf.int32, shape=(), initializer=tf.zeros_initializer())\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(tf_loss, global_step=tf_global_step)\n",
    "    return optimizer, tf_global_step\n",
    "\n",
    "def build_predict(tf_logit):\n",
    "    \"\"\"\n",
    "    Convert from tensor logit to tensor one hot\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def training_block(graph, tf_X, tf_y, tf_optimizer, tf_global_step, training_generator, test_generator):\n",
    "    \n",
    "    with graph.as_default() as gr:\n",
    "        tf_all_summary = tf.summary.merge_all()\n",
    "        \n",
    "        current_dir = os.getcwd()\n",
    "        experiment_name = datetime.strftime(datetime.now(), '%Y-%m-%dT%H:%M:%S')\n",
    "        tf_train_writer = tf.summary.FileWriter(logdir=os.path.join(current_dir, 'summary', 'train_' + experiment_name), graph=graph)\n",
    "        tf_test_writer = tf.summary.FileWriter(logdir=os.path.join(current_dir, 'summary', 'test_' + experiment_name), graph=graph)\n",
    "        \n",
    "        with tf.Session().as_default() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            SUMMARY_STEP = 10\n",
    "            EVALUATION_STEP = 10\n",
    "            for X, y in training_generator:\n",
    "                feed_dict = {tf_X: X, tf_y: y}\n",
    "                _, global_step = sess.run([tf_optimizer, tf_global_step], feed_dict=feed_dict)\n",
    "                \n",
    "                if global_step % SUMMARY_STEP == 0:\n",
    "                    logging.debug('Collect summary data at step: %s', global_step)\n",
    "                    train_summary_data = sess.run(tf_all_summary, feed_dict=feed_dict)\n",
    "                    tf_train_writer.add_summary(train_summary_data, global_step=global_step)\n",
    "                    \n",
    "                if global_step % EVALUATION_STEP == 0:\n",
    "                    logging.debug('Evaluate at step: %s', global_step)\n",
    "                    X_test, y_test = next(test_generator)\n",
    "                    \n",
    "                    test_summary_data = sess.run(tf_all_summary, feed_dict={\n",
    "                        tf_X: X_test,\n",
    "                        tf_y: y_test\n",
    "                    })\n",
    "                    tf_test_writer.add_summary(test_summary_data, global_step=global_step)\n",
    "\n",
    "def get_training_generator():\n",
    "    X = np.random.randint(1000, size=(512, 150))\n",
    "    y = np.random.randint(3, size=(512))\n",
    "    for i in range(1000):\n",
    "        yield X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_X, tf_y = build_input_v1()\n",
    "    tf_logit = build_inference_v1(tf_X)\n",
    "    tf_loss = build_loss_v1(tf_logit, tf_y)\n",
    "    tf_optimizer, tf_global_step = build_optimize_v1(tf_loss)\n",
    "    \n",
    "    training_block(graph=graph, tf_X=tf_X, tf_y=tf_y, training_generator=get_training_generator(), \n",
    "                   test_generator=get_training_generator(),\n",
    "                   tf_optimizer=tf_optimizer,\n",
    "                   tf_global_step=tf_global_step)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inference_v2(tf_X):\n",
    "    \"\"\"\n",
    "    ```Thoi kho qua, de thu sau\n",
    "    Return tensor logit\n",
    "    tf_X: [batch_size, sentence_max_length]\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.device('/cpu:0'), tf.variable_scope('embedding'):\n",
    "        VOCAB_SIZE = 10000\n",
    "        EMBEDDING_SIZE = 300\n",
    "        \n",
    "        tf_word_embeddings = tf.get_variable(name='word_embeddings', dtype=tf.float32,\n",
    "                                          shape=[VOCAB_SIZE, EMBEDDING_SIZE],\n",
    "                                          initializer=tf.truncated_normal_initializer(stddev=5e-2))\n",
    "        tf_projected_sentences = tf.nn.embedding_lookup(params=tf_word_embeddings, ids=tf_X)\n",
    "        list_tf_word_embeddings = tf.unstack(tf_projected_sentences, axis=1)\n",
    "        \n",
    "    with tf.variable_scope('LSTM'):\n",
    "        STATE_SIZE = 200\n",
    "        lstm_cell = rnn.BasicLSTMCell(STATE_SIZE, forget_bias=1.0)\n",
    "        # Each output has shape of [batch_size, state_size]\n",
    "        list_outputs, _ = rnn.static_rnn(cell=lstm_cell, inputs=word_embeddings, dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('Attention'):\n",
    "        ATTENTION_SIZE = 200\n",
    "        tf_output = tf.stack(list_outputs, axis=2) # [batch_size, state_size, sentence_max_length]\n",
    "        \n",
    "#         tf_attention_weights =\n",
    "        \n",
    "        tf_after_attention = tf.layers.dense(tf_output, units=ATTENTION_SIZE, activation=tf.nn.relu)\n",
    "        \n",
    "    with tf.variable_scope('Fully-Connected'):\n",
    "        tf_logit = tf.layers.dense(tf_after_attention, units=ATTENTION_SIZE, activation=tf.nn.relu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

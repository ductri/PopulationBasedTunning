{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from datetime import datetime\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "import nltk\n",
    "import collections\n",
    "import pickle\n",
    "import re\n",
    "from text2vector import Text2Vector\n",
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_PATH = os.path.join('/dataset', 'entropy_2018')\n",
    "TRAINING_PATH = os.path.join(ENTROPY_PATH, 'training_set.csv')\n",
    "TEST_PATH = os.path.join(ENTROPY_PATH, 'test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(doc):\n",
    "        doc = doc.lower()\n",
    "        NUMBERS_PATTERN = re.compile(r\"[+-]?\\d+(?:\\.\\d+)?\")\n",
    "        doc = re.sub(NUMBERS_PATTERN, '', doc)\n",
    "        URL_PATTERN = re.compile(\n",
    "                r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))')\n",
    "        doc = re.sub(URL_PATTERN, 'URL', doc)\n",
    "        return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load text2vector object from saved pickle\n",
      "INFO:root:Load dataset from pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.exists('text2vec.p'):\n",
    "    logging.info('Load text2vector object from saved pickle')\n",
    "    text2vec_model = pickle.load(open('text2vec.p', 'rb'))\n",
    "else:\n",
    "    logging.info('Fitting')\n",
    "    df_train = pd.read_csv(TRAINING_PATH)\n",
    "    docs = df_train['sentence'].map(preprocess_text)\n",
    "    text2vec_model = Text2Vector()\n",
    "    text2vec_model.fit(docs)\n",
    "    pickle.dump(text2vec_model, open('text2vec.p', 'wb' ))\n",
    "\n",
    "\n",
    "if os.path.exists('dataset.p'):\n",
    "    logging.info('Load dataset from pickle')\n",
    "    dataset = Dataset.from_pickle_file('dataset.p')\n",
    "else:\n",
    "    logging.info('Load dataset from CSV')\n",
    "    LABEL_MAPPING = {\n",
    "        'positive': 0,\n",
    "        'neutral': 1,\n",
    "        'negative': 2\n",
    "    }\n",
    "    def digitize_datapoint(datapoint):\n",
    "        doc, label = datapoint\n",
    "        doc = preprocess_text(doc)\n",
    "        return text2vec_model.doc_to_vec([doc])[0], LABEL_MAPPING[label]\n",
    "\n",
    "    text_dataset = Dataset.from_csv(TRAINING_PATH)\n",
    "    dataset = text_dataset.map(digitize_datapoint)\n",
    "    dataset.save('dataset.p')\n",
    "\n",
    "dataset = dataset.shuffle(10000)\n",
    "dataset = dataset.padded_batch(batch_size=2, list_lengths=(150, None), padded_value=text2vec_model.vocab_to_int[Text2Vector.PADDING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_db = Dataset.from_csv(TRAINING_PATH)\n",
    "# text_db = text_db.shuffle(10000)\n",
    "db = Dataset.from_pickle_file('dataset.p')\n",
    "# db = db.shuffle(10000)\n",
    "db = db.padded_batch(batch_size=1, list_lengths=(150, None), padded_value=text2vec_model.vocab_to_int[Text2Vector.PADDING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter1 = text_db.get_iterator()\n",
    "iter2 = db.get_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Xin chân thành cảm ơn nhà mạng  MobiFone July Six added 2 new photos — with trương thuật and 5 others. Photos from July Six's post\",\n",
       "  'neutral')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"xin chân thành cảm ơn nhà mạng mobifone july OUT_OF_VOCAB added new photos — with trương thuật and others . photos from july OUT_OF_VOCAB 's post PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING PADDING\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec_model.vec_to_doc([next(iter2)[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        filenames = [TRAINING_PATH]\n",
    "        record_defaults = [tf.string, tf.string]\n",
    "        dataset = tf.contrib.data.CsvDataset(filenames, record_defaults, header=True)\n",
    "        dataset = dataset.map(lambda doc, label: tuple(tf.py_func(preprocess_datapoint, [doc, label], [tf.int64, tf.int64])))\n",
    "        dataset = dataset.padded_batch(4, padded_shapes=(1, 1))\n",
    "\n",
    "#         dataset = dataset.batch(4)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "        x = sess.run(next_element)\n",
    "        y = sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(y, collections.Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec.vec_to_doc(text2vec.doc_to_vec(['duc tri nguyen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_v2():\n",
    "    \"\"\"\n",
    "    Return tensor input\n",
    "    \"\"\"\n",
    "    SENTENCE_MAX_LENGTH = 150\n",
    "    BATCH_SIZE = 2\n",
    "    \n",
    "    filenames = [TRAINING_PATH]\n",
    "    record_defaults = [tf.string, tf.string]\n",
    "    dataset = tf.contrib.data.CsvDataset(filenames, record_defaults, header=True)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_text)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    tf_X, tf_y = iterator.get_next()\n",
    "\n",
    "    return tf_X, tf_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### -----------------------------------------------------------------\n",
    "###           INGRADIENTS: atomic elements\n",
    "### -----------------------------------------------------------------\n",
    "def build_input_v1():\n",
    "    \"\"\"\n",
    "    Return tensor input\n",
    "    \"\"\"\n",
    "    SENTENCE_MAX_LENGTH = 150\n",
    "    tf_X = tf.placeholder(dtype=tf.int32, name='tf_X', shape=[None, SENTENCE_MAX_LENGTH])\n",
    "    tf_y = tf.placeholder(dtype=tf.int32, name='tf_y', shape=[None])\n",
    "    return tf_X, tf_y\n",
    "\n",
    "def build_inference_v1(tf_X):\n",
    "    def project(tf_X):\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('embedding'):\n",
    "            VOCAB_SIZE = 10000\n",
    "            EMBEDDING_SIZE = 300\n",
    "\n",
    "            tf_word_embeddings = tf.get_variable(name='word_embeddings', dtype=tf.float32,\n",
    "                                              shape=[VOCAB_SIZE, EMBEDDING_SIZE],\n",
    "                                              initializer=tf.truncated_normal_initializer(stddev=5e-2))\n",
    "            tf_projected_sentences = tf.nn.embedding_lookup(params=tf_word_embeddings, ids=tf_X)\n",
    "            return tf_projected_sentences\n",
    "    \n",
    "    tf_projected_sens = project(tf_X)\n",
    "    tf_projected_sens = tf.expand_dims(tf_projected_sens, axis=3)\n",
    "    \n",
    "    with tf.variable_scope('convolution_layer'):\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_projected_sens, filters=10, kernel_size=(5, 5), strides=(2, 2), padding='SAME', name='conv1')\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_after_conv, filters=20, kernel_size=(3, 3), strides=(2, 2), padding='SAME', name='conv2')\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        tf_flatten = tf.layers.flatten(tf_after_conv)\n",
    "        tf_logits = tf.layers.dense(inputs=tf_flatten, units=3, activation=tf.nn.relu)\n",
    "    \n",
    "    return tf_logits\n",
    "    \n",
    "def build_loss_v1(tf_logits, tf_y):\n",
    "    tf_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_y, logits=tf_logits)\n",
    "    tf_aggregated_loss = tf.reduce_mean(tf_losses)\n",
    "\n",
    "    tf.summary.scalar(name='loss', tensor=tf_aggregated_loss)\n",
    "    return tf_aggregated_loss\n",
    "\n",
    "def build_optimize_v1(tf_loss):\n",
    "    \"\"\"\n",
    "    Return tensor optimizer and global step\n",
    "    \"\"\"\n",
    "    tf_global_step = tf.get_variable(name='global_step', dtype=tf.int32, shape=(), initializer=tf.zeros_initializer())\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(tf_loss, global_step=tf_global_step)\n",
    "    return optimizer, tf_global_step\n",
    "\n",
    "def build_predict(tf_logit):\n",
    "    \"\"\"\n",
    "    Convert from tensor logit to tensor one hot\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def training_block(graph, tf_X, tf_y, tf_optimizer, tf_global_step, training_generator, test_generator):\n",
    "    \n",
    "    with graph.as_default() as gr:\n",
    "        tf_all_summary = tf.summary.merge_all()\n",
    "        \n",
    "        current_dir = os.getcwd()\n",
    "        experiment_name = datetime.strftime(datetime.now(), '%Y-%m-%dT%H:%M:%S')\n",
    "        tf_train_writer = tf.summary.FileWriter(logdir=os.path.join(current_dir, 'summary', 'train_' + experiment_name), graph=graph)\n",
    "        tf_test_writer = tf.summary.FileWriter(logdir=os.path.join(current_dir, 'summary', 'test_' + experiment_name), graph=graph)\n",
    "        \n",
    "        with tf.Session().as_default() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            SUMMARY_STEP = 10\n",
    "            EVALUATION_STEP = 10\n",
    "            for X, y in training_generator:\n",
    "                feed_dict = {tf_X: X, tf_y: y}\n",
    "                _, global_step = sess.run([tf_optimizer, tf_global_step], feed_dict=feed_dict)\n",
    "                \n",
    "                if global_step % SUMMARY_STEP == 0:\n",
    "                    logging.debug('Collect summary data at step: %s', global_step)\n",
    "                    train_summary_data = sess.run(tf_all_summary, feed_dict=feed_dict)\n",
    "                    tf_train_writer.add_summary(train_summary_data, global_step=global_step)\n",
    "                    \n",
    "                if global_step % EVALUATION_STEP == 0:\n",
    "                    logging.debug('Evaluate at step: %s', global_step)\n",
    "                    X_test, y_test = next(test_generator)\n",
    "                    \n",
    "                    test_summary_data = sess.run(tf_all_summary, feed_dict={\n",
    "                        tf_X: X_test,\n",
    "                        tf_y: y_test\n",
    "                    })\n",
    "                    tf_test_writer.add_summary(test_summary_data, global_step=global_step)\n",
    "\n",
    "def get_training_generator():\n",
    "    X = np.random.randint(1000, size=(512, 150))\n",
    "    y = np.random.randint(3, size=(512))\n",
    "    for i in range(1000):\n",
    "        yield X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_X, tf_y = build_input_v1()\n",
    "    tf_logit = build_inference_v1(tf_X)\n",
    "    tf_loss = build_loss_v1(tf_logit, tf_y)\n",
    "    tf_optimizer, tf_global_step = build_optimize_v1(tf_loss)\n",
    "    \n",
    "    training_block(graph=graph, tf_X=tf_X, tf_y=tf_y, training_generator=get_training_generator(), \n",
    "                   test_generator=get_training_generator(),\n",
    "                   tf_optimizer=tf_optimizer,\n",
    "                   tf_global_step=tf_global_step)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inference_v2(tf_X):\n",
    "    \"\"\"\n",
    "    ```Thoi kho qua, de thu sau\n",
    "    Return tensor logit\n",
    "    tf_X: [batch_size, sentence_max_length]\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.device('/cpu:0'), tf.variable_scope('embedding'):\n",
    "        VOCAB_SIZE = 10000\n",
    "        EMBEDDING_SIZE = 300\n",
    "        \n",
    "        tf_word_embeddings = tf.get_variable(name='word_embeddings', dtype=tf.float32,\n",
    "                                          shape=[VOCAB_SIZE, EMBEDDING_SIZE],\n",
    "                                          initializer=tf.truncated_normal_initializer(stddev=5e-2))\n",
    "        tf_projected_sentences = tf.nn.embedding_lookup(params=tf_word_embeddings, ids=tf_X)\n",
    "        list_tf_word_embeddings = tf.unstack(tf_projected_sentences, axis=1)\n",
    "        \n",
    "    with tf.variable_scope('LSTM'):\n",
    "        STATE_SIZE = 200\n",
    "        lstm_cell = rnn.BasicLSTMCell(STATE_SIZE, forget_bias=1.0)\n",
    "        # Each output has shape of [batch_size, state_size]\n",
    "        list_outputs, _ = rnn.static_rnn(cell=lstm_cell, inputs=word_embeddings, dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('Attention'):\n",
    "        ATTENTION_SIZE = 200\n",
    "        tf_output = tf.stack(list_outputs, axis=2) # [batch_size, state_size, sentence_max_length]\n",
    "        \n",
    "#         tf_attention_weights =\n",
    "        \n",
    "        tf_after_attention = tf.layers.dense(tf_output, units=ATTENTION_SIZE, activation=tf.nn.relu)\n",
    "        \n",
    "    with tf.variable_scope('Fully-Connected'):\n",
    "        tf_logit = tf.layers.dense(tf_after_attention, units=ATTENTION_SIZE, activation=tf.nn.relu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

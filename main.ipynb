{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from datetime import datetime\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_PATH = os.path.join('/dataset', 'entropy_2018')\n",
    "TRAINING_PATH = os.path.join(ENTROPY_PATH, 'training_set.csv')\n",
    "TEST_PATH = os.path.join(ENTROPY_PATH, 'test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAINING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(list_docs: List[str]) -> List[str]:\n",
    "    return list_docs\n",
    "\n",
    "def transform_label(y: List[str]) -> List[int]:\n",
    "    return []\n",
    "\n",
    "def filter_data(X: list, y: list):\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def proprocessing(docs, labels):\n",
    "    docs = transform_text(docs)\n",
    "    docs, labels = filter_data(docs, labels)\n",
    "    return docs, labels\n",
    "\n",
    "class Docs2Vec:\n",
    "    def __init__():\n",
    "        self.counts = None\n",
    "        self.int_to_vocab = None\n",
    "        self.vocab_to_int = None\n",
    "        \n",
    "    def fit(docs: List[str]):\n",
    "        logging.debug('-- From fit')\n",
    "        if self.counts or self.vocab_to_int or self.int_to_vocab:\n",
    "            raise Exception('\"fit\" is a one-time function')\n",
    "\n",
    "        list_tokenized_texts = [self.__tokenize(text) for text in list_texts]\n",
    "        all_tokens = itertools.chain(*list_tokenized_texts)\n",
    "        self.counts = collections.Counter(all_tokens)\n",
    "\n",
    "        self.int_to_vocab = self.__get_vocab()\n",
    "        self.int_to_vocab = [Text2Vector.PADDING] + self.int_to_vocab\n",
    "        self.vocab_to_int = {word: index for index, word in enumerate(self.int_to_vocab)}\n",
    "    \n",
    "    def docs2vec(docs: List[str]) -> List[List[int]]:\n",
    "        return [[]]\n",
    "\n",
    "    def labels2int(labels: List[str]) -> List[int]:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### -----------------------------------------------------------------\n",
    "###           INGRADIENTS: atomic elements\n",
    "### -----------------------------------------------------------------\n",
    "def build_input_v1():\n",
    "    \"\"\"\n",
    "    Return tensor input\n",
    "    \"\"\"\n",
    "    SENTENCE_MAX_LENGTH = 150\n",
    "    tf_X = tf.placeholder(dtype=tf.int32, name='tf_X', shape=[None, SENTENCE_MAX_LENGTH])\n",
    "    tf_y = tf.placeholder(dtype=tf.int32, name='tf_y', shape=[None])\n",
    "    return tf_X, tf_y\n",
    "\n",
    "def build_inference_v1(tf_X):\n",
    "    def project(tf_X):\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('embedding'):\n",
    "            VOCAB_SIZE = 10000\n",
    "            EMBEDDING_SIZE = 300\n",
    "\n",
    "            tf_word_embeddings = tf.get_variable(name='word_embeddings', dtype=tf.float32,\n",
    "                                              shape=[VOCAB_SIZE, EMBEDDING_SIZE],\n",
    "                                              initializer=tf.truncated_normal_initializer(stddev=5e-2))\n",
    "            tf_projected_sentences = tf.nn.embedding_lookup(params=tf_word_embeddings, ids=tf_X)\n",
    "            return tf_projected_sentences\n",
    "    \n",
    "    tf_projected_sens = project(tf_X)\n",
    "    tf_projected_sens = tf.expand_dims(tf_projected_sens, axis=3)\n",
    "    \n",
    "    with tf.variable_scope('convolution_layer'):\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_projected_sens, filters=10, kernel_size=(5, 5), strides=(2, 2), padding='SAME', name='conv1')\n",
    "        tf_after_conv = tf.layers.conv2d(inputs=tf_after_conv, filters=20, kernel_size=(3, 3), strides=(2, 2), padding='SAME', name='conv2')\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        tf_flatten = tf.layers.flatten(tf_after_conv)\n",
    "        tf_logits = tf.layers.dense(inputs=tf_flatten, units=3, activation=tf.nn.relu)\n",
    "    \n",
    "    return tf_logits\n",
    "    \n",
    "def build_loss_v1(tf_logits, tf_y):\n",
    "    tf_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_y, logits=tf_logits)\n",
    "    tf_aggregated_loss = tf.reduce_mean(tf_losses)\n",
    "\n",
    "    tf.summary.scalar(name='loss', tensor=tf_aggregated_loss)\n",
    "    return tf_aggregated_loss\n",
    "\n",
    "def build_optimize_v1(tf_loss):\n",
    "    \"\"\"\n",
    "    Return tensor optimizer and global step\n",
    "    \"\"\"\n",
    "    tf_global_step = tf.get_variable(name='global_step', dtype=tf.int32, shape=(), initializer=tf.zeros_initializer())\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(tf_loss, global_step=tf_global_step)\n",
    "    return optimizer, tf_global_step\n",
    "\n",
    "def build_predict(tf_logit):\n",
    "    \"\"\"\n",
    "    Convert from tensor logit to tensor one hot\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def training_block(graph, tf_X, tf_y, tf_optimizer, tf_global_step, training_generator, test_generator):\n",
    "    \n",
    "    with graph.as_default() as gr:\n",
    "        tf_all_summary = tf.summary.merge_all()\n",
    "        \n",
    "        current_dir = os.getcwd()\n",
    "        experiment_name = datetime.strftime(datetime.now(), '%Y-%m-%dT%H:%M:%S')\n",
    "        tf_train_writer = tf.summary.FileWriter(logdir=os.path.join(current_dir, 'summary', 'train_' + experiment_name), graph=graph)\n",
    "        tf_test_writer = tf.summary.FileWriter(logdir=os.path.join(current_dir, 'summary', 'test_' + experiment_name), graph=graph)\n",
    "        \n",
    "        with tf.Session().as_default() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            SUMMARY_STEP = 10\n",
    "            EVALUATION_STEP = 10\n",
    "            for X, y in training_generator:\n",
    "                feed_dict = {tf_X: X, tf_y: y}\n",
    "                _, global_step = sess.run([tf_optimizer, tf_global_step], feed_dict=feed_dict)\n",
    "                \n",
    "                if global_step % SUMMARY_STEP == 0:\n",
    "                    logging.debug('Collect summary data at step: %s', global_step)\n",
    "                    train_summary_data = sess.run(tf_all_summary, feed_dict=feed_dict)\n",
    "                    tf_train_writer.add_summary(train_summary_data, global_step=global_step)\n",
    "                    \n",
    "                if global_step % EVALUATION_STEP == 0:\n",
    "                    logging.debug('Evaluate at step: %s', global_step)\n",
    "                    X_test, y_test = next(test_generator)\n",
    "                    \n",
    "                    test_summary_data = sess.run(tf_all_summary, feed_dict={\n",
    "                        tf_X: X_test,\n",
    "                        tf_y: y_test\n",
    "                    })\n",
    "                    tf_test_writer.add_summary(test_summary_data, global_step=global_step)\n",
    "\n",
    "def get_training_generator():\n",
    "    X = np.random.randint(1000, size=(512, 150))\n",
    "    y = np.random.randint(3, size=(512))\n",
    "    for i in range(1000):\n",
    "        yield X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Collect summary data at step: 10\n",
      "DEBUG:root:Evaluate at step: 10\n",
      "DEBUG:root:Collect summary data at step: 20\n",
      "DEBUG:root:Evaluate at step: 20\n",
      "DEBUG:root:Collect summary data at step: 30\n",
      "DEBUG:root:Evaluate at step: 30\n",
      "DEBUG:root:Collect summary data at step: 40\n",
      "DEBUG:root:Evaluate at step: 40\n",
      "DEBUG:root:Collect summary data at step: 50\n",
      "DEBUG:root:Evaluate at step: 50\n",
      "DEBUG:root:Collect summary data at step: 60\n",
      "DEBUG:root:Evaluate at step: 60\n",
      "DEBUG:root:Collect summary data at step: 70\n",
      "DEBUG:root:Evaluate at step: 70\n",
      "DEBUG:root:Collect summary data at step: 80\n",
      "DEBUG:root:Evaluate at step: 80\n",
      "DEBUG:root:Collect summary data at step: 90\n",
      "DEBUG:root:Evaluate at step: 90\n",
      "DEBUG:root:Collect summary data at step: 100\n",
      "DEBUG:root:Evaluate at step: 100\n",
      "DEBUG:root:Collect summary data at step: 110\n",
      "DEBUG:root:Evaluate at step: 110\n",
      "DEBUG:root:Collect summary data at step: 120\n",
      "DEBUG:root:Evaluate at step: 120\n",
      "DEBUG:root:Collect summary data at step: 130\n",
      "DEBUG:root:Evaluate at step: 130\n",
      "DEBUG:root:Collect summary data at step: 140\n",
      "DEBUG:root:Evaluate at step: 140\n",
      "DEBUG:root:Collect summary data at step: 150\n",
      "DEBUG:root:Evaluate at step: 150\n",
      "DEBUG:root:Collect summary data at step: 160\n",
      "DEBUG:root:Evaluate at step: 160\n",
      "DEBUG:root:Collect summary data at step: 170\n",
      "DEBUG:root:Evaluate at step: 170\n",
      "DEBUG:root:Collect summary data at step: 180\n",
      "DEBUG:root:Evaluate at step: 180\n",
      "DEBUG:root:Collect summary data at step: 190\n",
      "DEBUG:root:Evaluate at step: 190\n",
      "DEBUG:root:Collect summary data at step: 200\n",
      "DEBUG:root:Evaluate at step: 200\n",
      "DEBUG:root:Collect summary data at step: 210\n",
      "DEBUG:root:Evaluate at step: 210\n",
      "DEBUG:root:Collect summary data at step: 220\n",
      "DEBUG:root:Evaluate at step: 220\n",
      "DEBUG:root:Collect summary data at step: 230\n",
      "DEBUG:root:Evaluate at step: 230\n",
      "DEBUG:root:Collect summary data at step: 240\n",
      "DEBUG:root:Evaluate at step: 240\n",
      "DEBUG:root:Collect summary data at step: 250\n",
      "DEBUG:root:Evaluate at step: 250\n",
      "DEBUG:root:Collect summary data at step: 260\n",
      "DEBUG:root:Evaluate at step: 260\n",
      "DEBUG:root:Collect summary data at step: 270\n",
      "DEBUG:root:Evaluate at step: 270\n",
      "DEBUG:root:Collect summary data at step: 280\n",
      "DEBUG:root:Evaluate at step: 280\n",
      "DEBUG:root:Collect summary data at step: 290\n",
      "DEBUG:root:Evaluate at step: 290\n",
      "DEBUG:root:Collect summary data at step: 300\n",
      "DEBUG:root:Evaluate at step: 300\n",
      "DEBUG:root:Collect summary data at step: 310\n",
      "DEBUG:root:Evaluate at step: 310\n",
      "DEBUG:root:Collect summary data at step: 320\n",
      "DEBUG:root:Evaluate at step: 320\n",
      "DEBUG:root:Collect summary data at step: 330\n",
      "DEBUG:root:Evaluate at step: 330\n",
      "DEBUG:root:Collect summary data at step: 340\n",
      "DEBUG:root:Evaluate at step: 340\n",
      "DEBUG:root:Collect summary data at step: 350\n",
      "DEBUG:root:Evaluate at step: 350\n",
      "DEBUG:root:Collect summary data at step: 360\n",
      "DEBUG:root:Evaluate at step: 360\n",
      "DEBUG:root:Collect summary data at step: 370\n",
      "DEBUG:root:Evaluate at step: 370\n",
      "DEBUG:root:Collect summary data at step: 380\n",
      "DEBUG:root:Evaluate at step: 380\n",
      "DEBUG:root:Collect summary data at step: 390\n",
      "DEBUG:root:Evaluate at step: 390\n",
      "DEBUG:root:Collect summary data at step: 400\n",
      "DEBUG:root:Evaluate at step: 400\n",
      "DEBUG:root:Collect summary data at step: 410\n",
      "DEBUG:root:Evaluate at step: 410\n",
      "DEBUG:root:Collect summary data at step: 420\n",
      "DEBUG:root:Evaluate at step: 420\n",
      "DEBUG:root:Collect summary data at step: 430\n",
      "DEBUG:root:Evaluate at step: 430\n",
      "DEBUG:root:Collect summary data at step: 440\n",
      "DEBUG:root:Evaluate at step: 440\n",
      "DEBUG:root:Collect summary data at step: 450\n",
      "DEBUG:root:Evaluate at step: 450\n",
      "DEBUG:root:Collect summary data at step: 460\n",
      "DEBUG:root:Evaluate at step: 460\n",
      "DEBUG:root:Collect summary data at step: 470\n",
      "DEBUG:root:Evaluate at step: 470\n",
      "DEBUG:root:Collect summary data at step: 480\n",
      "DEBUG:root:Evaluate at step: 480\n",
      "DEBUG:root:Collect summary data at step: 490\n",
      "DEBUG:root:Evaluate at step: 490\n",
      "DEBUG:root:Collect summary data at step: 500\n",
      "DEBUG:root:Evaluate at step: 500\n",
      "DEBUG:root:Collect summary data at step: 510\n",
      "DEBUG:root:Evaluate at step: 510\n",
      "DEBUG:root:Collect summary data at step: 520\n",
      "DEBUG:root:Evaluate at step: 520\n",
      "DEBUG:root:Collect summary data at step: 530\n",
      "DEBUG:root:Evaluate at step: 530\n",
      "DEBUG:root:Collect summary data at step: 540\n",
      "DEBUG:root:Evaluate at step: 540\n",
      "DEBUG:root:Collect summary data at step: 550\n",
      "DEBUG:root:Evaluate at step: 550\n",
      "DEBUG:root:Collect summary data at step: 560\n",
      "DEBUG:root:Evaluate at step: 560\n",
      "DEBUG:root:Collect summary data at step: 570\n",
      "DEBUG:root:Evaluate at step: 570\n",
      "DEBUG:root:Collect summary data at step: 580\n",
      "DEBUG:root:Evaluate at step: 580\n",
      "DEBUG:root:Collect summary data at step: 590\n",
      "DEBUG:root:Evaluate at step: 590\n",
      "DEBUG:root:Collect summary data at step: 600\n",
      "DEBUG:root:Evaluate at step: 600\n",
      "DEBUG:root:Collect summary data at step: 610\n",
      "DEBUG:root:Evaluate at step: 610\n",
      "DEBUG:root:Collect summary data at step: 620\n",
      "DEBUG:root:Evaluate at step: 620\n",
      "DEBUG:root:Collect summary data at step: 630\n",
      "DEBUG:root:Evaluate at step: 630\n",
      "DEBUG:root:Collect summary data at step: 640\n",
      "DEBUG:root:Evaluate at step: 640\n",
      "DEBUG:root:Collect summary data at step: 650\n",
      "DEBUG:root:Evaluate at step: 650\n",
      "DEBUG:root:Collect summary data at step: 660\n",
      "DEBUG:root:Evaluate at step: 660\n",
      "DEBUG:root:Collect summary data at step: 670\n",
      "DEBUG:root:Evaluate at step: 670\n",
      "DEBUG:root:Collect summary data at step: 680\n",
      "DEBUG:root:Evaluate at step: 680\n",
      "DEBUG:root:Collect summary data at step: 690\n",
      "DEBUG:root:Evaluate at step: 690\n",
      "DEBUG:root:Collect summary data at step: 700\n",
      "DEBUG:root:Evaluate at step: 700\n",
      "DEBUG:root:Collect summary data at step: 710\n",
      "DEBUG:root:Evaluate at step: 710\n",
      "DEBUG:root:Collect summary data at step: 720\n",
      "DEBUG:root:Evaluate at step: 720\n",
      "DEBUG:root:Collect summary data at step: 730\n",
      "DEBUG:root:Evaluate at step: 730\n",
      "DEBUG:root:Collect summary data at step: 740\n",
      "DEBUG:root:Evaluate at step: 740\n",
      "DEBUG:root:Collect summary data at step: 750\n",
      "DEBUG:root:Evaluate at step: 750\n",
      "DEBUG:root:Collect summary data at step: 760\n",
      "DEBUG:root:Evaluate at step: 760\n",
      "DEBUG:root:Collect summary data at step: 770\n",
      "DEBUG:root:Evaluate at step: 770\n",
      "DEBUG:root:Collect summary data at step: 780\n",
      "DEBUG:root:Evaluate at step: 780\n",
      "DEBUG:root:Collect summary data at step: 790\n",
      "DEBUG:root:Evaluate at step: 790\n",
      "DEBUG:root:Collect summary data at step: 800\n",
      "DEBUG:root:Evaluate at step: 800\n",
      "DEBUG:root:Collect summary data at step: 810\n",
      "DEBUG:root:Evaluate at step: 810\n",
      "DEBUG:root:Collect summary data at step: 820\n",
      "DEBUG:root:Evaluate at step: 820\n",
      "DEBUG:root:Collect summary data at step: 830\n",
      "DEBUG:root:Evaluate at step: 830\n",
      "DEBUG:root:Collect summary data at step: 840\n",
      "DEBUG:root:Evaluate at step: 840\n",
      "DEBUG:root:Collect summary data at step: 850\n",
      "DEBUG:root:Evaluate at step: 850\n",
      "DEBUG:root:Collect summary data at step: 860\n",
      "DEBUG:root:Evaluate at step: 860\n",
      "DEBUG:root:Collect summary data at step: 870\n",
      "DEBUG:root:Evaluate at step: 870\n",
      "DEBUG:root:Collect summary data at step: 880\n",
      "DEBUG:root:Evaluate at step: 880\n",
      "DEBUG:root:Collect summary data at step: 890\n",
      "DEBUG:root:Evaluate at step: 890\n",
      "DEBUG:root:Collect summary data at step: 900\n",
      "DEBUG:root:Evaluate at step: 900\n",
      "DEBUG:root:Collect summary data at step: 910\n",
      "DEBUG:root:Evaluate at step: 910\n",
      "DEBUG:root:Collect summary data at step: 920\n",
      "DEBUG:root:Evaluate at step: 920\n",
      "DEBUG:root:Collect summary data at step: 930\n",
      "DEBUG:root:Evaluate at step: 930\n",
      "DEBUG:root:Collect summary data at step: 940\n",
      "DEBUG:root:Evaluate at step: 940\n",
      "DEBUG:root:Collect summary data at step: 950\n",
      "DEBUG:root:Evaluate at step: 950\n",
      "DEBUG:root:Collect summary data at step: 960\n",
      "DEBUG:root:Evaluate at step: 960\n",
      "DEBUG:root:Collect summary data at step: 970\n",
      "DEBUG:root:Evaluate at step: 970\n",
      "DEBUG:root:Collect summary data at step: 980\n",
      "DEBUG:root:Evaluate at step: 980\n",
      "DEBUG:root:Collect summary data at step: 990\n",
      "DEBUG:root:Evaluate at step: 990\n",
      "DEBUG:root:Collect summary data at step: 1000\n",
      "DEBUG:root:Evaluate at step: 1000\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_X, tf_y = build_input_v1()\n",
    "    tf_logit = build_inference_v1(tf_X)\n",
    "    tf_loss = build_loss_v1(tf_logit, tf_y)\n",
    "    tf_optimizer, tf_global_step = build_optimize_v1(tf_loss)\n",
    "    \n",
    "    training_block(graph=graph, tf_X=tf_X, tf_y=tf_y, training_generator=get_training_generator(), \n",
    "                   test_generator=get_training_generator(),\n",
    "                   tf_optimizer=tf_optimizer,\n",
    "                   tf_global_step=tf_global_step)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inference_v2(tf_X):\n",
    "    \"\"\"\n",
    "    ```Thoi kho qua, de thu sau\n",
    "    Return tensor logit\n",
    "    tf_X: [batch_size, sentence_max_length]\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.device('/cpu:0'), tf.variable_scope('embedding'):\n",
    "        VOCAB_SIZE = 10000\n",
    "        EMBEDDING_SIZE = 300\n",
    "        \n",
    "        tf_word_embeddings = tf.get_variable(name='word_embeddings', dtype=tf.float32,\n",
    "                                          shape=[VOCAB_SIZE, EMBEDDING_SIZE],\n",
    "                                          initializer=tf.truncated_normal_initializer(stddev=5e-2))\n",
    "        tf_projected_sentences = tf.nn.embedding_lookup(params=tf_word_embeddings, ids=tf_X)\n",
    "        list_tf_word_embeddings = tf.unstack(tf_projected_sentences, axis=1)\n",
    "        \n",
    "    with tf.variable_scope('LSTM'):\n",
    "        STATE_SIZE = 200\n",
    "        lstm_cell = rnn.BasicLSTMCell(STATE_SIZE, forget_bias=1.0)\n",
    "        # Each output has shape of [batch_size, state_size]\n",
    "        list_outputs, _ = rnn.static_rnn(cell=lstm_cell, inputs=word_embeddings, dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('Attention'):\n",
    "        ATTENTION_SIZE = 200\n",
    "        tf_output = tf.stack(list_outputs, axis=2) # [batch_size, state_size, sentence_max_length]\n",
    "        \n",
    "#         tf_attention_weights =\n",
    "        \n",
    "        tf_after_attention = tf.layers.dense(tf_output, units=ATTENTION_SIZE, activation=tf.nn.relu)\n",
    "        \n",
    "    with tf.variable_scope('Fully-Connected'):\n",
    "        tf_logit = tf.layers.dense(tf_after_attention, units=ATTENTION_SIZE, activation=tf.nn.relu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

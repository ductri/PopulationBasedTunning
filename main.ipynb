{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from datetime import datetime\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "import nltk\n",
    "import collections\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from text2vector import Text2Vector\n",
    "from dataset import Dataset\n",
    "from random import shuffle\n",
    "import ingradient\n",
    "import utils\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_PATH = os.path.join('/dataset', 'entropy_2018')\n",
    "TRAINING_PATH = os.path.join(ENTROPY_PATH, 'training_set.csv')\n",
    "TEST_PATH = os.path.join(ENTROPY_PATH, 'test_set.csv')\n",
    "SAMPLE_PATH = os.path.join(ENTROPY_PATH, 'sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(doc):\n",
    "    doc = doc.lower()\n",
    "    NUMBERS_PATTERN = re.compile(r\"[+-]?\\d+(?:\\.\\d+)?\")\n",
    "    doc = re.sub(NUMBERS_PATTERN, '', doc)\n",
    "    URL_PATTERN = re.compile(\n",
    "            r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))')\n",
    "    doc = re.sub(URL_PATTERN, 'URL', doc)\n",
    "    return doc\n",
    "\n",
    "if os.path.exists('text2vec.p'):\n",
    "    logging.info('Load text2vector object from saved pickle')\n",
    "    text2vec_model = Text2Vector.load('text2vec.p')\n",
    "else:\n",
    "    logging.info('Fitting')\n",
    "    df_train = pd.read_csv(TRAINING_PATH)\n",
    "    docs = df_train['sentence'].map(preprocess_text)\n",
    "    text2vec_model = Text2Vector()\n",
    "    text2vec_model.fit(docs)\n",
    "    text2vec_model.save('text2vec.p')\n",
    "\n",
    "\n",
    "if os.path.exists('training_dataset.p'):\n",
    "    logging.info('Load training dataset from pickle')\n",
    "    training_dataset = Dataset.load('training_dataset.p')\n",
    "else:\n",
    "    logging.info('Load training dataset from CSV')\n",
    "    LABEL_MAPPING = {\n",
    "        'positive': 0,\n",
    "        'neutral': 1,\n",
    "        'negative': 2\n",
    "    }\n",
    "    def digitize_datapoint(datapoint):\n",
    "        doc, label = datapoint\n",
    "        doc = preprocess_text(doc)\n",
    "        return text2vec_model.doc_to_vec([doc])[0], LABEL_MAPPING[label]\n",
    "\n",
    "    training_dataset = Dataset.from_csv(TRAINING_PATH)\n",
    "    training_dataset = training_dataset.map(digitize_datapoint)\n",
    "    training_dataset.save('training_dataset.p')\n",
    "\n",
    "    \n",
    "if os.path.exists('test_dataset.p'):\n",
    "    logging.info('Load test dataset from pickle')\n",
    "    test_dataset = Dataset.load('test_dataset.p')\n",
    "else:\n",
    "    logging.info('Load test dataset from CSV')\n",
    "    LABEL_MAPPING = {\n",
    "        'positive': 0,\n",
    "        'neutral': 1,\n",
    "        'negative': 2\n",
    "    }\n",
    "    def digitize_datapoint(datapoint):\n",
    "        doc, label = datapoint\n",
    "        doc = preprocess_text(doc)\n",
    "        return text2vec_model.doc_to_vec([doc])[0], LABEL_MAPPING[label]\n",
    "\n",
    "    test_dataset = Dataset.from_csv(TEST_PATH)\n",
    "    test_dataset = test_dataset.map(digitize_datapoint)\n",
    "    test_dataset.save('test_dataset.p')\n",
    "    \n",
    "BATCH_SIZE = 128\n",
    "training_dataset = training_dataset.shuffle(10000)\n",
    "training_dataset = training_dataset.padded_batch(batch_size=BATCH_SIZE, list_lengths=(150, None), padded_value=text2vec_model.vocab_to_int[Text2Vector.PADDING])\n",
    "training_dataset = training_dataset.repeat(300)\n",
    "\n",
    "test_dataset = test_dataset.shuffle(10000)\n",
    "test_dataset = test_dataset.padded_batch(batch_size=1000, list_lengths=(150, None), padded_value=text2vec_model.vocab_to_int[Text2Vector.PADDING])\n",
    "test_dataset = test_dataset.repeat(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_X, tf_y = ingradient.build_input_v1()\n",
    "    tf_logit = ingradient.build_inference_v1(tf_X)\n",
    "    tf_predict = ingradient.build_predict_v1(tf_logit)\n",
    "    ingradient.build_accuracy_v1(tf_predict, tf_y)\n",
    "    tf_loss = ingradient.build_loss_v1(tf_logit, tf_y)\n",
    "    tf_optimizer, tf_global_step = ingradient.build_optimize_v1(tf_loss)\n",
    "    logging.info('Total parameters: %s', utils.count_trainable_variables())\n",
    "    ingradient.training_block(graph=graph, tf_X=tf_X, tf_y=tf_y, training_generator=training_dataset.get_iterator(), \n",
    "                   test_generator=test_dataset.get_iterator(),\n",
    "                   tf_optimizer=tf_optimizer,\n",
    "                   tf_global_step=tf_global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_data_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
